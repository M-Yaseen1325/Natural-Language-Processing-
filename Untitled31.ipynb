{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "5mTf_y-M7L1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030d5589-6db9-4901-9b59-84feecd1147a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 66
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yruNTBAz1gha",
        "outputId": "08b84d74-8554-411a-db8d-a0644d2dcb58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note on NLP Project\n",
            "Project Title: Understanding and Building Projects Using Language Data\n",
            "\n",
            "Overview:\n",
            "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling\n",
            "machines to understand, interpret, and respond to human language. \n",
            "The project involves exploring various NLP techniques, building applications that work with text data, and \n",
            "leveraging models to perform tasks such as sentiment analysis, text summarization, or chatbotÂ development.\n",
            "\n",
            "Key Objectives:\n",
            "1:Tools and Frameworks\n",
            "2:Potential Use Cases\n",
            "3:Learning Outcomes\n",
            "4:Challenges\n"
          ]
        }
      ],
      "source": [
        "# Open the text file\n",
        "text_file = open(\"/content/NLP.txt\")\n",
        "\n",
        "# Read the content of the text file\n",
        "text = text_file.read()\n",
        "\n",
        "# Print the content of the text file\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6crXZuh3zln",
        "outputId": "1a9adef3-a2f1-48b9-a2bb-fc4de1a4eee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling machines to understand interpret and respond to human language.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Example text\n",
        "text = \"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling machines to understand interpret and respond to human language.\"\n",
        "\n",
        "# Tokenizing the text into sentences\n",
        "sentence_tokenized = sent_tokenize(text)\n",
        "\n",
        "# Printing the tokenized sentences\n",
        "print(sentence_tokenized)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the function\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Example text\n",
        "text = \"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling machines to understand interpret and respond to human language.\"\n",
        "\n",
        "# Tokenizing the text into words\n",
        "text_tokenized = word_tokenize(text)\n",
        "\n",
        "# Printing the result\n",
        "print(text_tokenized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkHAn9bFFIw-",
        "outputId": "a94bfb7b-84bf-4ea9-ad14-fe5d21a975b0"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'enabling', 'machines', 'to', 'understand', 'interpret', 'and', 'respond', 'to', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "\n",
        "# Example text\n",
        "text = \"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling machines to understand interpret and respond to human language.\"\n",
        "\n",
        "# Tokenizing the text into words\n",
        "word_tokenized = word_tokenize(text)\n",
        "\n",
        "# Creating a frequency distribution\n",
        "freq_dist = FreqDist(word_tokenized)\n",
        "\n",
        "# Printing the frequency distribution\n",
        "print(freq_dist)\n",
        "\n",
        "# Printing the most common words\n",
        "print(freq_dist.most_common())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltY5fWrrFPfz",
        "outputId": "21a40760-f129-4b15-cbd0-946d83cc2162"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 25 samples and 26 outcomes>\n",
            "[('to', 2), ('Natural', 1), ('Language', 1), ('Processing', 1), ('(', 1), ('NLP', 1), (')', 1), ('is', 1), ('a', 1), ('field', 1), ('of', 1), ('artificial', 1), ('intelligence', 1), ('that', 1), ('focuses', 1), ('on', 1), ('enabling', 1), ('machines', 1), ('understand', 1), ('interpret', 1), ('and', 1), ('respond', 1), ('human', 1), ('language', 1), ('.', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Example text\n",
        "text = \"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling machines to understand interpret and respond to human language.\"\n",
        "\n",
        "# Lowercasing the text\n",
        "text = text.lower()\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Initializing tools for word tokenization, stop words removal, and lemmatization\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Processing each sentence\n",
        "processed_sentences = []\n",
        "for sentence in sentences:\n",
        "    # Word tokenization\n",
        "    words = word_tokenize(sentence)\n",
        "    # Removing punctuation and stop words, and lemmatizing\n",
        "    filtered_words = [\n",
        "        lemmatizer.lemmatize(word)\n",
        "        for word in words\n",
        "        if word not in stop_words and word not in string.punctuation\n",
        "    ]\n",
        "    # Adding processed words back as a sentence\n",
        "    processed_sentences.append(\" \".join(filtered_words))\n",
        "\n",
        "# Printing the processed sentences\n",
        "print(processed_sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zq5redhmLzh",
        "outputId": "a5325c39-b0ad-4d9b-b5eb-8d37cc0cf313"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural language processing nlp field artificial intelligence focus enabling machine understand interpret respond human language']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How can traning this data\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources (only needed once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses text by lowercasing, tokenizing, removing stop words and punctuation, and lemmatizing.\n",
        "\n",
        "    Args:\n",
        "      text: The input text string.\n",
        "\n",
        "    Returns:\n",
        "      A list of processed sentences.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    sentences = sent_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    processed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        filtered_words = [\n",
        "            lemmatizer.lemmatize(word)\n",
        "            for word in words\n",
        "            if word not in stop_words and word not in string.punctuation\n",
        "        ]\n",
        "        processed_sentences.append(\" \".join(filtered_words))\n",
        "    return processed_sentences\n",
        "\n",
        "\n",
        "def train_model(processed_text):\n",
        "    \"\"\"\n",
        "    Trains a basic frequency distribution model.  This is a placeholder, and more sophisticated models would be needed for real-world tasks.\n",
        "\n",
        "    Args:\n",
        "        processed_text: A list of preprocessed sentences.\n",
        "\n",
        "    Returns:\n",
        "        A frequency distribution object.\n",
        "    \"\"\"\n",
        "\n",
        "    all_words = []\n",
        "    for sentence in processed_text:\n",
        "        all_words.extend(word_tokenize(sentence))  # Tokenize each sentence again\n",
        "    freq_dist = FreqDist(all_words)\n",
        "    return freq_dist\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "with open(\"/content/NLP.txt\", \"r\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "processed_text = preprocess_text(text)\n",
        "print(\"Processed text:\")\n",
        "print(processed_text) # Output the processed data\n",
        "\n",
        "frequency_distribution = train_model(processed_text)\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "print(frequency_distribution.most_common(20))  # Show top 20 most frequent words\n",
        "# or any analysis you would want to perform on the data."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3CHh76_zf1k",
        "outputId": "e8884ec3-cbe7-4724-a4df-da6d3cd22d7d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed text:\n",
            "['note nlp project project title understanding building project using language data overview natural language processing nlp field artificial intelligence focus enabling machine understand interpret respond human language', 'project involves exploring various nlp technique building application work text data leveraging model perform task sentiment analysis text summarization chatbot development', 'key objective 1 tool framework 2 potential use case 3 learning outcome 4 challenge']\n",
            "\n",
            "Frequency Distribution:\n",
            "[('project', 4), ('nlp', 3), ('language', 3), ('building', 2), ('data', 2), ('text', 2), ('note', 1), ('title', 1), ('understanding', 1), ('using', 1), ('overview', 1), ('natural', 1), ('processing', 1), ('field', 1), ('artificial', 1), ('intelligence', 1), ('focus', 1), ('enabling', 1), ('machine', 1), ('understand', 1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Precision and Recall: Precision calculates the percentage of correct positive predictions among all positive predictions made by the chatbot. Recall calculates the percentage of correct positive predictions among all actual positive instances. These metrics help assess the chatbot's ability to correctly identify relevant information.\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources (only needed once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses text by lowercasing, tokenizing, removing stop words and punctuation, and lemmatizing.\n",
        "\n",
        "    Args:\n",
        "      text: The input text string.\n",
        "\n",
        "    Returns:\n",
        "      A list of processed sentences.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    sentences = sent_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    processed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        filtered_words = [\n",
        "            lemmatizer.lemmatize(word)\n",
        "            for word in words\n",
        "            if word not in stop_words and word not in string.punctuation\n",
        "        ]\n",
        "        processed_sentences.append(\" \".join(filtered_words))\n",
        "    return processed_sentences\n",
        "\n",
        "\n",
        "def calculate_response_accuracy(user_query, chatbot_response, reference_text):\n",
        "    \"\"\"\n",
        "    Calculates precision and recall of the chatbot's response.\n",
        "\n",
        "    Args:\n",
        "        user_query: The user's input query.\n",
        "        chatbot_response: The chatbot's response.\n",
        "        reference_text: The reference text that contains the correct information.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing precision and recall, or None if processing fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        processed_response = set(word_tokenize(\" \".join(preprocess_text(chatbot_response))))\n",
        "        processed_reference = set(word_tokenize(\" \".join(preprocess_text(reference_text))))\n",
        "\n",
        "        true_positives = len(processed_response.intersection(processed_reference))\n",
        "\n",
        "        if not processed_response:  # Handle cases where the response is empty\n",
        "          precision = 0.0\n",
        "        else:\n",
        "          precision = true_positives / len(processed_response)\n",
        "\n",
        "        if not processed_reference: # Handle cases where the reference is empty\n",
        "            recall = 0.0\n",
        "        else:\n",
        "          recall = true_positives / len(processed_reference)\n",
        "\n",
        "        return {\"precision\": precision, \"recall\": recall}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during accuracy calculation: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example Usage\n",
        "user_query = \"What is the capital of France?\"\n",
        "chatbot_response = \"The capital of France is Paris.\"\n",
        "reference_text = \"Paris is the capital city of France, known for its iconic Eiffel Tower.\"\n",
        "\n",
        "\n",
        "metrics = calculate_response_accuracy(user_query, chatbot_response, reference_text)\n",
        "\n",
        "if metrics:\n",
        "  print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "  print(f\"Recall: {metrics['recall']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kosJWEG1Rzy",
        "outputId": "ca3cc2c8-a172-45b0-d339-fc0c2870fa69"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0000\n",
            "Recall: 0.3750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_churn_rate(user_sessions):\n",
        "    \"\"\"\n",
        "    Calculates the churn rate based on user sessions.\n",
        "\n",
        "    Args:\n",
        "      user_sessions: A list of dictionaries, where each dictionary represents a user session\n",
        "        and contains keys for user ID and session end date.\n",
        "\n",
        "    Returns:\n",
        "      The churn rate as a decimal (e.g. 0.10 for 10%).\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if not user_sessions:\n",
        "        return 0.0  # Handle empty session data\n",
        "\n",
        "    churned_users = 0\n",
        "    for session in user_sessions:\n",
        "      # Simulate a threshold for churn (e.g., inactive for 30 days)\n",
        "        if (datetime.now() - session['end_date']).days > 30:\n",
        "            churned_users += 1\n",
        "\n",
        "    total_users = len(user_sessions)\n",
        "    churn_rate = churned_users / total_users if total_users > 0 else 0.0\n",
        "    return churn_rate\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Example Usage (Simulate some session data)\n",
        "\n",
        "user_sessions = [\n",
        "    {'user_id': '1', 'end_date': datetime.now() - timedelta(days=10)},\n",
        "    {'user_id': '2', 'end_date': datetime.now() - timedelta(days=45)},\n",
        "    {'user_id': '3', 'end_date': datetime.now() - timedelta(days=5)},\n",
        "    {'user_id': '4', 'end_date': datetime.now() - timedelta(days=60)},\n",
        "    {'user_id': '5', 'end_date': datetime.now() - timedelta(days=2)}\n",
        "]\n",
        "\n",
        "churn_rate = calculate_churn_rate(user_sessions)\n",
        "print(f\"Churn Rate: {churn_rate:.4f}\")\n",
        "\n",
        "\n",
        "# The human handoff rate is the percentage of conversations that require intervention from a human agent. This metric provides insights into when the chatbot fails to provide a satisfactory response, requiring escalation to a human agent.\n",
        "\n",
        "def calculate_human_handoff_rate(conversation_log):\n",
        "    \"\"\"Calculates the human handoff rate from a conversation log.\"\"\"\n",
        "    try:\n",
        "        handoffs = 0\n",
        "        total_conversations = len(conversation_log)\n",
        "\n",
        "        for conversation in conversation_log:\n",
        "          if \"human_handoff\" in conversation:\n",
        "            handoffs += conversation[\"human_handoff\"]\n",
        "\n",
        "        if total_conversations == 0:\n",
        "          return 0.0\n",
        "\n",
        "        handoff_rate = handoffs / total_conversations\n",
        "        return handoff_rate\n",
        "\n",
        "    except KeyError:\n",
        "        print(\"Error: Conversation logs do not contain 'human_handoff' key.\")\n",
        "        return None\n",
        "\n",
        "# Example usage (replace with actual conversation log data)\n",
        "\n",
        "conversation_log = [\n",
        "    {\"conversation_id\": 1, \"human_handoff\": 0},  # No handoff\n",
        "    {\"conversation_id\": 2, \"human_handoff\": 1},  # Handoff occurred\n",
        "    {\"conversation_id\": 3, \"human_handoff\": 0},\n",
        "    {\"conversation_id\": 4, \"human_handoff\": 1},\n",
        "]\n",
        "\n",
        "\n",
        "handoff_rate = calculate_human_handoff_rate(conversation_log)\n",
        "\n",
        "if handoff_rate is not None:\n",
        "  print(f\"Human Handoff Rate: {handoff_rate:.4f}\")\n",
        "\n",
        "# Example sentiment analysis integration (using TextBlob)\n",
        "from textblob import TextBlob\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "  \"\"\"Analyzes the sentiment of the given text using TextBlob.\"\"\"\n",
        "  analysis = TextBlob(text)\n",
        "  return analysis.sentiment.polarity\n",
        "\n",
        "# Example usage\n",
        "user_feedback = \"I'm very happy with the chatbot's performance. It was helpful.\"\n",
        "sentiment = analyze_sentiment(user_feedback)\n",
        "print(f\"User Feedback Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lIadQ0a2Dpk",
        "outputId": "85da36ef-3aae-4aa1-e159-0245469ceafc"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Churn Rate: 0.4000\n",
            "Human Handoff Rate: 0.5000\n",
            "User Feedback Sentiment: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task Completion Rate: Measures the percentage of user tasks or goals that the chatbot successfully assists with or completes\n",
        "def task_completion_rate(user_interactions):\n",
        "    \"\"\"\n",
        "    Calculates the task completion rate based on user interactions.\n",
        "\n",
        "    Args:\n",
        "    user_interactions: A list of dictionaries, where each dictionary represents\n",
        "                        a user interaction and contains a 'success' key\n",
        "                        indicating whether the task was completed (True/False).\n",
        "\n",
        "    Returns:\n",
        "    The task completion rate as a decimal (e.g. 0.85 for 85%).\n",
        "    Returns 0.0 if the input list is empty to avoid division by zero.\n",
        "    \"\"\"\n",
        "\n",
        "    if not user_interactions:\n",
        "        return 0.0  # Handle empty input gracefully\n",
        "\n",
        "    completed_tasks = sum(1 for interaction in user_interactions if interaction['success'])\n",
        "    total_tasks = len(user_interactions)\n",
        "    return completed_tasks / total_tasks\n",
        "\n",
        "\n",
        "# Example usage (replace with actual user interaction data)\n",
        "user_interactions = [\n",
        "    {'task': 'book_flight', 'success': True},\n",
        "    {'task': 'check_weather', 'success': True},\n",
        "    {'task': 'find_restaurant', 'success': False},\n",
        "    {'task': 'set_alarm', 'success': True},\n",
        "]\n",
        "\n",
        "completion_rate = task_completion_rate(user_interactions)\n",
        "print(f\"Task Completion Rate: {completion_rate:.2%}\") # Formatted as a percentage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwy6HTId3G_c",
        "outputId": "10a6be25-2e36-4b6d-bbe1-7003fde991b0"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task Completion Rate: 75.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The model should be tested on a separate test set of conversations that was not used for training, to ensure that it can generalize to new data.\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from datetime import datetime, timedelta\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Download necessary NLTK resources (only needed once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocesses text for analysis.\"\"\"\n",
        "    text = text.lower()\n",
        "    sentences = sent_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        filtered_words = [\n",
        "            lemmatizer.lemmatize(word)\n",
        "            for word in words\n",
        "            if word not in stop_words and word not in string.punctuation\n",
        "        ]\n",
        "        processed_sentences.append(\" \".join(filtered_words))\n",
        "    return processed_sentences\n",
        "\n",
        "\n",
        "def calculate_response_accuracy(user_query, chatbot_response, reference_text):\n",
        "    \"\"\"Calculates precision and recall of the chatbot's response.\"\"\"\n",
        "    try:\n",
        "        processed_response = set(word_tokenize(\" \".join(preprocess_text(chatbot_response))))\n",
        "        processed_reference = set(word_tokenize(\" \".join(preprocess_text(reference_text))))\n",
        "        true_positives = len(processed_response.intersection(processed_reference))\n",
        "\n",
        "        if not processed_response:\n",
        "            precision = 0.0\n",
        "        else:\n",
        "            precision = true_positives / len(processed_response)\n",
        "\n",
        "        if not processed_reference:\n",
        "            recall = 0.0\n",
        "        else:\n",
        "            recall = true_positives / len(processed_reference)\n",
        "\n",
        "        return {\"precision\": precision, \"recall\": recall}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during accuracy calculation: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Example test cases (replace with your actual test data)\n",
        "test_cases = [\n",
        "    {\n",
        "        \"user_query\": \"What is the capital of France?\",\n",
        "        \"chatbot_response\": \"The capital of France is Paris.\",\n",
        "        \"reference_text\": \"Paris is the capital city of France, known for its iconic Eiffel Tower.\"\n",
        "    },\n",
        "    {\n",
        "        \"user_query\": \"How's the weather today?\",\n",
        "        \"chatbot_response\": \"It's sunny with a high of 75 degrees.\",\n",
        "        \"reference_text\": \"Expect sunny skies today with temperatures reaching 75 degrees Fahrenheit.\"\n",
        "    },\n",
        "    # Add more test cases here...\n",
        "]\n",
        "\n",
        "\n",
        "for i, test_case in enumerate(test_cases):\n",
        "    metrics = calculate_response_accuracy(test_case[\"user_query\"], test_case[\"chatbot_response\"], test_case[\"reference_text\"])\n",
        "    if metrics:\n",
        "        print(f\"Test Case {i+1}:\")\n",
        "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
        "        print(f\"  Recall: {metrics['recall']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7MDFrJT7EV1",
        "outputId": "f0af8286-2511-4c20-a6fd-85a9ef9cdae8"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Case 1:\n",
            "  Precision: 1.0000\n",
            "  Recall: 0.3750\n",
            "Test Case 2:\n",
            "  Precision: 0.6000\n",
            "  Recall: 0.3333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}